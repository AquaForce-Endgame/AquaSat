---
title: "05_sr_matchups"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(feather)
library(knitr)
library(purrr)
library(reticulate)
library(googledrive)
opts_knit$set(root.dir='../..')

```

This script takes the shouldered, filtered in situ water quality dataset pulled from the Water Quality Portal/LAGOS and extracts associated surface atmosphere reflectance values from Google Earth Engine.  The pulled reflectance values are the median pixel value of pixels with 80% water occurence (according to Pekel) within 120m of the provided lat/long of the in situ sample identified in the water quality dataset.

*Reticulate uses the default python path for the local machine.  This python needs to be authorized with google earth engine and have the necessary packages installed.*  
```{r Get Existing}

###Download the split wqp/lagos data to send up to GGE.  This is easier in R than python. Also create a list of existing reflectance csv's to filter whats been pulled down from gge already.

splits <- drive_ls('watersat/2_rsdata/out/SplitWide/')

##546 is the number of splitwide files as of 04/11/18, this may need to be changed if the wqp/lagos data changes.

if(length(list.files('2_rsdata/out/SplitWide')) < 546){
  for(i in 1:nrow(splits)) {
    path = paste0(getwd(),'/2_rsdata/out/SplitWide/', splits$name[i])
    drive_download(as_id(splits$id[i]),
                   path=path, overwrite = TRUE)
  }
}

filesDownR <- drive_ls("watersat/2_rsdata/out/sr_matchups/") %>% 
  select(name)

```

```{r Google Earth Engine Pull}
##repl_python basically starts a python bash window within your R chunk.  This can be used to actually interact with earth engine.

repl_python()
import time
import ee
import os
import feather
ee.Initialize()

#Source necessary functions.
execfile('2_rsdata/src/5a_6a_GEE_pull_functions.py')

#Load in Pekel water occurance Layer and Landsat Collections.

pekel = ee.Image('JRC/GSW1_0/GlobalSurfaceWater')

l8 = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')
l7 = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')
l5 = ee.ImageCollection('LANDSAT/LT05/C01/T1_SR')

#Identify toa for use in sourced functions.
collection = 'SR'

#Standardize band names between the various collections and aggregate 
#them into one image collection

bn8 = ['B2','B3', 'B4', 'B5', 'B6','B7', 'BQA']
bn57 = ['B1', 'B2', 'B3', 'B4', 'B5','B7', 'BQA']
bns = ['Blue', 'Green', 'Red', 'Nir', 'Swir1', 'Swir2', 'qa']
  
ls5 = l5.select(bn57, bns)
ls7 = l7.select(bn57, bns)
ls8 = l8.select(bn8, bns)

ls = ee.ImageCollection(ls5.merge(ls7).merge(ls8))

#Selct the occurence layer in the pekel mask, which is just the 
#percentage of water occurence over a given pixel from 1985-2015.
#Set the percent occurance threshold and create a watermask from the result.
threshold = 80
water = pekel.select('occurrence').gt(threshold)
water = water.updateMask(water)

## Set buffer distance of pixels to include in median calculation.  Distance is in meters from supplied sample point.  
dist = 120

## Identify folder with in-situ data broken up into 5000 observation chunks by path/row and
ULdir = '2_rsdata/out/SplitWide/'

#Generate file lists for both files to be sent and files already processed in google earth engine
filesDown = r.filesDownR['name']
filesUp = os.listdir(ULdir) 

#Remove hidden formatting file in directory and any file names in filesUp that are already in files down.
filesUp  = filter(lambda x: x  != '.DS_Store', filesUp)
filesDown = [i.replace(".csv", ".feather") for i in filesDown]

filesUpFlt = [x for x in filesUp if x not in filesDown]


#### Wrap it all up in a for loop running through our list of files

for x in range(0,len(filesUpFlt)):

  #Read in our file as a feather data frame
  inv = feather.read_dataframe(ULdir + filesUpFlt[x])
  #turn our inventory into a feature collection by assigning 
  #lat longs and a site id.  Do this via list comprehension 
  #(similar to for loop but faster and apparently plays nice with earth engine.)
  invOut = ee.FeatureCollection([ee.Feature(ee.Geometry.Point([inv['long'][i],\
  inv['lat'][i]]),{'SiteID':inv['SiteID'][i],  'Date':inv['Date'][i],'SampDate':inv['SampDate'][i]}) for i in range(0,len(inv))]) 
  
  #Pull out the path/row from the file name
  path = int(filesUpFlt[x].replace('.','_').split('_')[0])
  row = int(filesUpFlt[x].replace('.','_').split('_')[1])
  
  #Filter image collection to path/row  
  lsover = ee.ImageCollection(ls.filter(ee.Filter.eq('WRS_PATH',\
  path)).filter(ee.Filter.eq('WRS_ROW', row)))
    
  ## Map over sites within specific path row
  data = ee.FeatureCollection(invOut.map(sitePull))
  
  #Extract path/row count Variable so names match up with file sent to GGE
  if filesUpFlt[x].replace('.','_').split('_')[2] == 'feather':
    count = ''
  else:
    count = '_'+str(int(filesUpFlt[x].replace('.','_').split('_')[2]))
  
  dataOut = ee.batch.Export.table.toDrive(collection = data, \
                                              description = str(path)\
                                               +'_'+str(row) + count,\
                                              folder = 'WQP_SR_MatchUps',\
                                              fileFormat = 'csv')
  maximum_no_of_tasks(15, 60)
  dataOut.start()

## End the python bash.
exit
```

```{r Update Repositories}
  
#Finally, Download the new data locally and share to team folder.
filesNew <- drive_ls('WQP_TOA_MatchUps')

for(i in 1:nrow(filesNew)) {
  path = paste0(getwd(),'/2_rsdata/out/toa_matchups/', filesNew$name[i])
  drive_download(as_id(filesNew$id[i]),
                 path=path, overwrite = TRUE)
}

for(i in 1:nrow(filesNew)) {
  file = paste0(getwd(),'/2_rsdata/out/toa_matchups/', filesNew$name[i])
  drive_upload(file, path = 'watersat/2_rsdata/out/toa_matchups/')
}

##Clean up non-team folder so no duplicates end up in it during future pulls
drive_rm(as_dribble(filesNew))

```

