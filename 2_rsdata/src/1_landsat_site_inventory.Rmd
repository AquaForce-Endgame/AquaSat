---
title: "FlatSiteInventory"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Subsetting Water Quality Portal sites to landsat visible pixels.

The water quality portal gives us a big inventory of sites with siteids and lat longs. Lots of these sites are on small rivers or have multiple characteristics associated with them. So we need to create a subset of the data that is only landsat visible. For now this involves some initial cleaning of the data in R. Sending the data to earth engine and then checking if the water pixels associated with the point are landsat visible (generally wider than 60meters)

Obviously this is not in the remake style yet. Future goals. 


```{r setup}
library(knitr)
library(feather)
library(googledrive)
library(tidyverse)
library(LAGOSNE)
library(reticulate)

opts_knit$set(root.dir='../..')


```

## Grab all unique water quality sites. 

Uniqueness is defined by a unique combination of site name and lat and long.

```{r}

#Read in inventory and exclude nutrient data (not a goal of this project just yet)
inv <- read_feather('1_wqdata/out/wqp_inventory.feather') %>%
  filter(!Constituent %in% c('tn','p','nitrate')) %>%
  filter(!StateCode %in% c('72','66')) %>% #No puerto rico or guam
  distinct(MonitoringLocationIdentifier,LatitudeMeasure,LongitudeMeasure) %>% #Keep only unique site lat and long. 
  filter(!is.na(LatitudeMeasure)) %>% #Exclude sites with missing geospatial data
  filter(!is.na(LongitudeMeasure)) %>%
  rename(SiteID=MonitoringLocationIdentifier,lat=LatitudeMeasure,long=LongitudeMeasure) %>%
  mutate(source='WQP')

#Load in lagos data
lagos.sites <- lagosne_load("1.087.1")$locus %>%
  select(SiteID = lagoslakeid,lat=nhd_lat,long=nhd_long) %>%
  mutate(SiteID = paste('lagos',SiteID,sep='-')) %>%
  mutate(source='LAGOS')

#Bind lagos and wqp data together
lagos.wqp <- bind_rows(lagos.sites,inv) %>%
  arrange(lat,long)

#Save this data. 
write_feather(lagos.wqp,'2_rsdata/tmp/unique_site_inventory.feather')



```


## Check if sites are visible as a water body in Landsat. 

Most of the data living in the Water Quality Portal is on streams, lakes, and/or estuaries that are too small to be visible as water bodies from Landsat satellites, which have a resolution of ~30m. In this project we only want to keep sites that are consistently close to areas classified as water. Luckily there is a really nice paper and dataset from Jean Francois Pekel and others, where they looked at the entire history of Landsat 5-8 and classified how often any given pixel was classified as water. Data from [this paper](https://www.nature.com/articles/nature20584) is available in GEE and we use it to only keep sites that are close to water bodies visible from landsat. 

The code below checks all sites for water pixels within 200m of the lat long of sites in the WQP. It takes 5 minutes or so to run with 400000 sites

```{r}
#Set.RProfile to set the default python for reticulate
Sys.setenv(RETICULATE_PYTHON = '/usr/local/bin/python2')

#Start python bash
repl_python()

#Import Libraries
import ee
ee.Initialize()
import pandas as p
import feather as f
import time

#Read in the water mask
pekel = ee.Image('JRC/GSW1_0/GlobalSurfaceWater')

#Point to the inventory feather
file = '2_rsdata/tmp/unique_site_inventory.feather'

#Read in our inventory feather
inv = f.read_dataframe(file)

#Get the length of unique sites
obs = len(inv.index) 

#To convert these sites into featurecollections we have to split it up 
#in a loop. 
#List comprehension loop requires exact indecis to convert site lat long
#into gee collections
splitStart = range(0, obs, 5000)
splitEnd=range(5000,obs + 5000 ,5000)
splitEnd[-1]=obs

#Selct the occurence layer in the pekel mask, which is just the percentage of water occurence
#over a given pixel from 1985-2015. 
occ = pekel.select('occurrence')


#####----------Earth Engine Pull here ---------######


#Define our water extraction function outside of the loop 
#so it is not defined over and over

def waterfunc(buf):
  #Define a 200m buffer around each point
  invBuf = buf.buffer(200).geometry()
  #Clip the pekel mask to this buffer
  pekclip = occ.clip(invBuf)
  #Reduce the buffer to pekel min and max
  pekMin = pekclip.reduceRegion(ee.Reducer.minMax(), invBuf, 30)
  #Add another reducer to get the median pekel occurnce
  pekMed = pekclip.reduceRegion(ee.Reducer.median(),invBuf,30)
  #Define the output features
  out = buf.set({'max':pekMin.get('occurrence_max')})\
          .set({'min':pekMin.get('occurrence_min')})\
          .set({'med':pekMed.get('occurrence')})
          
  return out

#Source function to limit number of tasks sent up to earth engine.
execfile('2_rsdata/src/5a_6a_GEE_pull_functions.py')
      
#Loop over the index stored in split
for x in range(0,len(splitStart)):
#turn our inventory into a feature collection by assigning lat longs and a site id.
#This is done via list comprehension which is similar to a for loop but faster and
#plays nice with earth engine.  Collections are limited for 5000 to avoid time outs
#on the server side.
  invOut = ee.FeatureCollection([ee.Feature(
    ee.Geometry.Point([inv['long'][i], inv['lat'][i]]),
    {'SiteID':inv['SiteID'][i]}) for i in range(splitStart[x], splitEnd[x])]) 
  #Map this function over the 5000 or so created sites
  outdata = invOut.map(waterfunc)
  #Define a data export 
  dataOut = ee.batch.Export.table.toDrive(collection = outdata,
                                          description = "LandsatSitePull" + str(x),
                                          folder='tempSiteWater',
                                          fileFormat = 'csv')
  #Send next task.
  dataOut.start()
#Make sure all Earth engine tasks are completed prior to moving on.  
maximum_no_of_tasks(1,60)
print('done')

## End the python bash.
exit
```



## Local download

```{r , eval=F}
#Download data from python output stored in local googledrive folder called tempSiteWater
folder <- drive_ls('tempSiteWater')


#Download files locally.
for(i in 1:nrow(folder)) {
  path=paste0('2_rsdata/tmp/water_visible_sites/',folder$name[i])
  drive_download(as_id(folder$id[i]),
                 path=path)
}

# Move files from personal drive folder into team drive folder. 
for(i in 1:nrow(folder)) {
  drive_mv(as_id(folder$id[i]), path = '2_rsdata/tmp/water_visible_sites/')
}


```

## Read in data and save as a single file

This produces a final water inventory if the max pekel occurence is greater than 60 (later steps can make this more strict).

```{r}
#Get a list of those files
water.inv.files <- list.files('2_rsdata/tmp/water_visible_sites',full.names = T)

#Use map_df to read them all in and stitch them together
water.inv <- map_df(water.inv.files,read_csv)

#Keep only sites with max pekel > 60 (this number is arbitrary and can be changed)
#Extract the coordinates from the .geo using str split
pekel.max <- 60
water.60 <- water.inv %>%
  filter(max > pekel.max) %>%
  mutate(coords = str_split_fixed(`.geo`,'\\[',2)[,2] %>%
           gsub(']}','',.),
        long = as.numeric(str_split_fixed(coords,',',2)[,1]),
        lat = as.numeric(str_split_fixed(coords,',',2)[,2])) %>%
  select(-`.geo`,-coords,-`system:index`)


write_feather(water.60,'2_rsdata/out/unique_site_visible_inv.feather')

#Upload to team drive
drive_upload('2_rsdata/out/unique_site_visible_inv.feather', 'watersat/2_rsdata/out/')

```


