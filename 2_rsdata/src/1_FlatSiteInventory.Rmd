---
title: "FlatSiteInventory"
output: html_document
editor_options: 
  chunk_output_type: inline
---

The water quality portal gives us a big inventory of sites with siteids and lat longs. Lots of these sites are on small rivers or have multiple characteristics associated with them. So we need to create a subset of the data that is only landsat visible. For now this involves some initial cleaning of the data in R. Sending the data to earth engine and then checking if the water pixels associated with the point are landsat visible (generally wider than 60meters)

Obviously this is not in the remake style yet. Future goals. 
```{r setup}
library(knitr)
opts_knit$set(root.dir='~/Dropbox/UNC-PostDoc All/aquasat/')
library(feather)
library(googledrive)
library(tidyverse)


```


```{r, eval=F}





#Read in inventory and exclude nutrient data (not a goal of this project just yet)
inv <- read_feather('1_wqdata/out/wqp_inventory.feather') %>%
  filter(!Constituent %in% c('tn','p','nitrate')) %>%
  filter(!StateCode %in% c('02','15','72','66')) %>% #No alaska, hawaii, or puerto rico or guam
  distinct(MonitoringLocationIdentifier,LatitudeMeasure,LongitudeMeasure) %>%
  filter(!is.na(LatitudeMeasure)) %>%
  filter(!is.na(LongitudeMeasure)) %>%
  rename(SiteID=MonitoringLocationIdentifier,lat=LatitudeMeasure,long=LongitudeMeasure)

#Load in lagos data
lagos.sites <- lagosne_load("1.087.1")$locus %>%
  select(SiteID = lagoslakeid,lat=nhd_lat,long=nhd_long) %>%
  mutate(SiteID = paste('lagos',SiteID,sep='-'))





lagos.wqp <- bind_rows(lagos.sites,inv)
#write_csv(inv,'2_rsdata/tmp/uniqueInventory.csv')
write_feather(lagos.wqp,'2_rsdata/tmp/uniqueInventory.feather')



```




#This takes 5 minutes or so to run with 400000 sites
```{python, eval=F, engine.path="/usr/local/bin/python2"}
import ee
ee.Initialize()
import pandas as p
import feather as f

#Read in the water mask
pekel = ee.Image('JRC/GSW1_0/GlobalSurfaceWater')

#Point to the inventory feather
file = '2_rsdata/tmp/uniqueInventory.feather'

#Read in our inventory feather
inv = f.read_dataframe(file)

#Get the length of unique sites
obs = len(inv.index) + 1

#To convert these sites into featurecollections we have to split it up 
#in a loop. I don't entirely understand why, but it works quickly and reproducibly.
split = range(0, obs, 5000)

#Selct the occurence layer in the pekel mask, which is just the percentage of water occurence
#over a given pixel from 1985-2015. 
occ = pekel.select('occurrence')


#####----------Earth Engine Pull here ---------######


#Define our water extraction function outside of the loop 
#so it is not defined over and over

def waterfunc(buf):
    #Define a 200m buffer around each point
    invBuf = buf.buffer(200).geometry()
    #Clip the pekel mask to this buffer
    pekclip = occ.clip(invBuf)
    #Reduce the buffer to pekel min and max
    pekMin = pekclip.reduceRegion(ee.Reducer.minMax(), invBuf, 30)
    #Add another reducer to get the median pekel occurnce
    pekMed = pekclip.reduceRegion(ee.Reducer.median(),invBuf,30)
    #Define the output features
    out = buf.set({'max':pekMin.get('occurrence_max')})\
            .set({'min':pekMin.get('occurrence_min')})\
            .set({'med':pekMed.get('occurrence')})
          
    return out
  
#Loop over the index stored in split
for x in split:

#turn our inventory into a feature collection by assigning lat longs and a site id.
#This is done via list comprehension which is similar to a for loop but faster and
#plays nice with earth engine.  Collections are limited for 5000 to avoid time outs
#on the server side.
  invOut = ee.FeatureCollection([ee.Feature(
            ee.Geometry.Point([inv['long'][i], inv['lat'][i]]),
              {'SiteID':inv['SiteID'][i]}) for i in range(x, x + 5000)]) 



  #Map this function over the 5000 or so created sites
  outdata = invOut.map(waterfunc)


  #Define a data export 
  dataOut = ee.batch.Export.table.toDrive(collection = outdata,
                        description = "LandsatSitePull" + str(x),
                        folder='tempSiteWater',
                        fileFormat = 'csv')
                        
  #Start that export
  dataOut.start()
  #Close the loop
print(done)
```



#Now download all that data to local machine 

```{r , eval=F}
library(googledrive)

folder <- drive_ls('tempSiteWater')

#Download the drive data

# A case for a for loop, because google only allows you one query at a time

for(i in 1:nrow(folder)) {
  path=paste0('2_rsdata/tmp/siteWater/',folder$name[i])
  drive_download(as_id(folder$id[i]),
                 path=path,
                 overwrite=T)
  if(file.size(path) == 0){
      drive_download(as_id(folder$id[i]),
                 path=path,
                 overwrite=T)
  }
}

```

#Read in all that data and save it as a final water inventory if the max pekel occurence is greater than 60 (later steps can make this more strict)
```{r}
#Get a list of those files
water.inv.files <- list.files('2_rsdata/tmp/siteWater',full.names = T)

#Use map_df to read them all in and stitch them together
water.inv <- map_df(water.inv.files,read_csv)

#Keep only sites with max pekel > 60 (this number is arbitrary and can be changed)
#Extract the coordinates from the .geo using str split
water.60 <- water.inv %>%
  filter(max > 60) %>%
  mutate(coords = str_split_fixed(`.geo`,'\\[',2)[,2] %>%
           gsub(']}','',.)) %>%
  mutate(lat = as.numeric(str_split_fixed(coords,',',2)[,2])) %>%
  mutate(long = as.numeric(str_split_fixed(coords,',',2)[,1])) %>%
  select(-`.geo`,-coords,-`system:index`)





write_feather(water.60,'2_rsdata/out/uniqueWaterInventory.feather')
```


